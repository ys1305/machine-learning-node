关联分析（一）：频繁项集及规则产生
=================

   关联分析用于发现隐藏在大型数据集中有意义的联系，属于模式挖掘分析方法，其为人熟知的经典应用当属沃尔玛超市里“啤酒与尿布”的关系挖掘了。关联分析的应用领域非常多，当数据集类型比较复杂时，进行关联分析采用的手段也相对复杂，本篇从最简单的事务数据集着手，对关联分析进行解读。 对大型事务数据集进行关联分析时，有两个问题要考虑：

*   发现关联模式时耗费的计算量
*   发现的关联模式是否可信

关联分析方法主要就是围绕这两个问题展开。

**1.基本概念**
----------

- 事务：每一条交易称为一个事务。例如，在这个例子中包含了5个事务。
- 项：交易的每一个物品称为一个项，例如豆奶、尿布等。
- 项集：包含零个或者多个项的集合叫做项集，例如{豆奶，莴苣}。
- k项集：包含k个项的项集叫做k一项集。例如{豆奶}叫做1项集，{豆奶，尿布，啤酒}叫做3项集。
- 前件和后件：对于规则{尿布}→{啤酒}，{尿布}叫做前件，{啤酒}叫做后件。

* 二元属性事务集

   在购物篮事务数据集中，每一条记录中属性只有购买(1)和不购买(0)两种情况，不统计商品的任何其他信息，如下所示

    交易号码商品
   001豆奶，莴苣
   002莴苣，尿布，啤酒，甜菜
   003豆奶，尿布，啤酒，橙汁
   004莴苣，豆奶，尿布，啤酒
   005莴苣，豆奶，尿布，橙汁


* 非对称二元变量

   对上面的商品购买记录来说，购买商品更加引人关注，人们一般不关注未购买的商品，这样的二元变量即称为非对称二元变量。

* 关联规则

   关联规则是形如![X\rightarrow Y](https://private.codecogs.com/gif.latex?X%5Crightarrow%20Y)的表达式，![X](https://private.codecogs.com/gif.latex?X)和![Y](https://private.codecogs.com/gif.latex?Y)是两个不相交的项集，这里的项集指的是购买商品的集合。![X](https://private.codecogs.com/gif.latex?X)称为规则前件，![Y](https://private.codecogs.com/gif.latex?Y)称为规则后件。

### 常用的频繁项集的评估标准

#### 支持度

支持度就是几个关联的数据在数据集中出现的次数占总数据集的比重。或者说几个数据关联出现的概率。如果我们有两个想分析关联性的数据X和Y，则对应的支持度为:
$$
Support(X, Y)=P(X Y)=\frac{\text { number }(X Y)}{\text {num(AllSamples } )}\\
Support(X, Y,Z)=P(X YZ)=\frac{\text { number }(X YZ)}{\text {num(AllSamples } )}
$$
一般来说，支持度高的数据不一定构成频繁项集，但是支持度太低的数据肯定不构成频繁项集。另外， 支持度是针对项集来说的，因此，可以定义一个最小支持度，而只保留满足最小支持度的项集，起到一个项集过滤的作用。

以上面事务集为例，假设![X](https://private.codecogs.com/gif.latex?X)表示啤酒，![Y](https://private.codecogs.com/gif.latex?Y)表示尿布，则啤酒![\rightarrow](https://private.codecogs.com/gif.latex?%5Crightarrow)尿布的支持度为

​                                                     ![s(X\rightarrow Y)=\frac{N(X,Y)}{N}=\frac{3}{5}=0.6](https://private.codecogs.com/gif.latex?s%28X%5Crightarrow%20Y%29%3D%5Cfrac%7BN%28X%2CY%29%7D%7BN%7D%3D%5Cfrac%7B3%7D%7B5%7D%3D0.6)

频繁项集

项集的支持度超过设定的阈值时，该项集即称为频繁项集。





#### 置信度

置信度体现了一个数据出现后，另一个数据出现的概率，或者说数据的条件概率。如果我们有两个想分析关联性的数据X和Y，X对Y的置信度为
$$
Confidence(X \Leftarrow Y)=P(X | Y)=P(X Y) / P(Y)
$$
以上面事务集为例，假设![X](https://private.codecogs.com/gif.latex?X)表示啤酒，![Y](https://private.codecogs.com/gif.latex?Y)表示尿布，则啤酒![\rightarrow](https://private.codecogs.com/gif.latex?%5Crightarrow)尿布的置信度为

​                                                    ![c(X\rightarrow Y)=\frac{N(Y|X)}{N(X)}=\frac{N(X,Y)}{N(X)}=\frac{3}{3}=1](https://private.codecogs.com/gif.latex?c%28X%5Crightarrow%20Y%29%3D%5Cfrac%7BN%28Y%7CX%29%7D%7BN%28X%29%7D%3D%5Cfrac%7BN%28X%2CY%29%7D%7BN%28X%29%7D%3D%5Cfrac%7B3%7D%7B3%7D%3D1)

#### 提升度

提升度表示含有Y的条件下，同时含有X的概率，与X总体发生的概率之比，即
$$
\operatorname{Lift}(X \Leftarrow Y)=P(X | Y) / P(X)=\text {Confidence}(X \Leftarrow Y) / P(X)
$$
提升度体先了X和Y之间的关联关系, 提升度大于1则$X \Leftarrow Y*$是有效的强关联规则， 提升度小于等于1则$X \Leftarrow Y$是无效的强关联规则 。一个特殊的情况，如果X和Y独立，则有$Lift(X \Leftarrow Y) = 1$，因为此时$P(X|Y) = P(X)$。

### 关联规则

给定事务的集合T，关联规则发现是指找出支持度大于等于minsup并且置信度大于等于minconf的所有规则，其中minsup和minconf是对应的支持度和置信度阈值。挖掘关联规则的一种原始方法是：计算每个可能规则的支持度和置信度。但是这种方法的代价很高，令人望而却步，因为可以从数据集提取的规则的数目达指数级。更具体地说，从包含d个项的数据集提取的可能规则的总数为：​                                        
$$
R=3^{d}-2^{d+1}+1
$$
对于我们前面聚德小例子，里面一共有6中商品，提取的可能规则数为：$3^6-2^7 + 1 = 602$，也就是说对于只有6种商品的小数据集都需要计算602条规则的支持度和置信度。使用minsup= 20%和minconf=50%，80%以上的规则将被丢弃，使得大部分计算是无用的开销。为了避免进行不必要的计算，事先对规则剪枝，而无须计算它们的支持度和置信度的值将是有益的。因此， 大多数关联规则挖掘算法通常采用的一种策略是，将关联规则挖掘任务分解为如下两个主要的子任务。

- 频繁项集产生：
    其目标是发现满足最小支持度阈值的所有项集，这些项集称作频繁项集（frequent itemset）。

- 规则的产生：
    其目标是从上一步发现的频繁项集中提取所有高置信度的规则，这些规则称作强规则（strong rule）。

    通常，频繁项集产生所需的计算开销远大于产生规则所需的计算开销。那有没有办法可以减少这种无用的计算呢？有。下面这两种方法可以降低产生频繁项集的计算复杂度：
    （1）减少候选项集的数目M。
    （2）减少比较次数。替代将每个候选项集与每个事务相匹配，可以使用更高级的数据结构，或者存储候选项集或者压缩数据集，来减少比较次数。

**2.频繁项集的产生**
-------------

在生成频繁项集时，可以从两个方向考虑来节省计算时间：

- 减少候选频繁项集

- 通过采用高级的数据结构，减少项集搜索时间。

### 2.1 Apriori算法中的频繁项集产生方法

  在Apriori算法中，用到了两条先验原理：

- 如果一个项集不是频繁项集，那么该项集的超集也必定不是频繁项集；

- 如果一个项集是频繁项集，那么该项集的子集也是频繁项集，利用这两条先验原理可以大大较少候选频繁项集的数量。

Apriori算法采用了迭代的方法，先搜索出候选1项集及对应的支持度，剪枝去掉低于支持度的1项集，得到频繁1项集。然后对剩下的频繁1项集进行连接，得到候选的频繁2项集，筛选去掉低于支持度的候选频繁2项集，得到真正的频繁二项集，以此类推，迭代下去，直到无法找到频繁k+1项集为止，对应的频繁k项集的集合即为算法的输出结果。

![1558253380936](C:\Users\Administrator.PC-201805312002\AppData\Roaming\Typora\typora-user-images\1558253380936.png)

![1558253320152](C:\Users\Administrator.PC-201805312002\AppData\Roaming\Typora\typora-user-images\1558253320152.png)

![1558253341125](C:\Users\Administrator.PC-201805312002\AppData\Roaming\Typora\typora-user-images\1558253341125.png)

对应${B,C,E}$重复计算三次的情况有对应的优化方法

#### 算法流程

输入：数据集合D，支持度阈值$\alpha$

输出：最大的频繁k项集

1）扫描整个数据集，得到所有出现过的数据，作为候选频繁1项集。k=1，频繁0项集为空集。

2）挖掘频繁k项集

a) 扫描数据计算候选频繁k项集的支持度

b) 去除候选频繁k项集中支持度低于阈值的数据集,得到频繁k项集。如果得到的频繁k项集为空，则直接返回频繁k-1项集的集合作为算法结果，算法结束。如果得到的频繁k项集只有一项，则直接返回频繁k项集的集合作为算法结果，算法结束。

c) 基于频繁k项集，连接生成候选频繁k+1项集。

3） 令k=k+1，转入步骤2。

从算法的步骤可以看出，Aprior算法每轮迭代都要扫描数据集，因此在数据集很大，数据种类很多的时候，算法效率很低。

#### 总结

- $C_k$中的项集是用来产生频集的候选集.
- 最后的频集$L_k$必须是$C_k$的一个子集。
    - $C_k$中的每个元素需在交易数据库中进行验证来决定其是否加
        入$L_k$

- 验证过程是性能瓶颈
    - 交易数据库可能非常大
    - 比如频集最多包含10个项，那么就需要扫描交易数据库10遍
    - 需要很大的I/O负载。

#### 2.1.1 候选项集 

在以上过程中，涉及到多次数据集的扫描，每次从候选频繁项集中找出频繁项集时都需要扫描一次数据集，而且还有另外一个问题，如何从$k-1$项频繁项集中生成候选$k$项频繁项集？

对该问题，可以采用![F_{k-1}\times F_{k-1}](https://private.codecogs.com/gif.latex?F_%7Bk-1%7D%5Ctimes%20F_%7Bk-1%7D)方法：

​     $F_{k-1}\times F_{k-1}$方法是通过合并一对 $k-1$项频繁项集生成候选 $k$项频繁项集，不过==要求这一对 $k-1$项频繁项集的前$k-2$个项相同==，但是有1-项频繁项集生成2-项候选频繁项集时不需如此。

举个例子，现在有两个3-项频繁项集 {面包，牛奶，啤酒}、{面包，牛奶，可乐}，那么这两个3-项频繁项集就可以生成一个4-项候选频繁项集 {面包，牛奶，啤酒，可乐}，但是{面包，牛奶，啤酒}和{面包，尿布，可乐}就无法按照此规则生成4-项候选频繁项集。按照这种方式生成候选频繁项集，有一个要求，**==数据集中的项必须先制定好排序，所有记录中的项需要按照该排序规则排列==。**为什么会采用这种方式生成候选频繁项集呢？还是以 {面包，牛奶，啤酒，可乐}为例，如果其为频繁项集，那么其子集也是频繁项集，因此{{面包，牛奶，啤酒}、 {面包，牛奶，可乐}均为频繁项集，基于这种原理能**减少候选频繁项集的数量**。

![img](http://images2015.cnblogs.com/blog/1042406/201701/1042406-20170117161036255-1753157633.png)

我们的数据集D有4条记录，分别是134,235,1235和25。现在我们用Apriori算法来寻找频繁k项集，最小支持度设置为50%。首先我们生成候选频繁1项集，包括我们所有的5个数据并计算5个数据的支持度，计算完毕后我们进行剪枝，数据4由于支持度只有25%被剪掉。我们最终的频繁1项集为1235，现在我们链接生成候选频繁2项集，包括12,13,15,23,25,35共6组。此时我们的第一轮迭代结束。

进入第二轮迭代，我们扫描数据集计算候选频繁2项集的支持度，接着进行剪枝，由于12和15的支持度只有25%而被筛除，得到真正的频繁2项集，包括13,23,25,35。

错误解释:现在我们链接生成候选频繁3项集,123, 125，135和235共4组，这部分图中没有画出。通过计算候选频繁3项集的支持度，我们发现123,125和135的支持度均为25%，因此接着被剪枝，最终得到的真正频繁3项集为235一组。由于此时我们无法再进行数据连接，进而得到候选频繁4项集，最终的结果即为频繁3三项集235。

==正确解释==:现在利用{2，3}、{3，5}、{3，5}来创建 三元素项集，如果两两组合的话，会得到{2，3，5}、 {2，3，5}、{2，3，5}三个一模一样的结果。也就是说，同样的结果集合会重复3次。而我们需要的是尽可能少的遍历列表。现在，如果比较集合{2，3}、{3，5}、{3，5}的第一个元素并只对第一个元素相同的集合 （即 {2，3}、{2，5}） 进行并集操作的话，就可以得到{2，3，5}， 在这个过程中就执行了一次操作！这样就不需要遍历列表来寻找非重复值啦。而{1,3}与{3,5}都没有第一个元素与之相同的2项集,故不进行操作==[原因是由于以1开头的其他2项集{1,2},{1,5}都不是频繁集]==

####  2.1.2 候选项集支持度计数  

  从候选频繁项集中筛选出频繁项集时需要对它们进行支持度计数。支持度计数的方法这里介绍两种，一种是线性扫描数据集，将数据集中的每一条记录与所有候选频繁项集进行匹配并计数，最终得到频繁项集。在候选频繁项集比较多的情况下，这种方法中进行比较的次数会较多，此时可以采用第二种方法；第二种方法中，使用Hash结构计数，**能较少比较次数**。实施过程如下：

* 例设数据集中包含的项的集合为 { 面包，牛奶，尿布，啤酒，鸡蛋，可乐}，并且顺序按此顺序排列，那么可以依次给它们分配标号为{1，2，3，4，5，6}。假设通过上文中介绍的内容我们得到的候选的3-项频繁项集为{1，2，3}、{1，3，5}、{1，3，6}、{2，4，6}、{2，5，6}、{3，4，6}，{3，5，6}、{4，5，6}，由于是3-项候选集，因此利用Hash函数$h(p)=p \: \:mod \: \:3$来建立3-项候选频繁项集的Hash树，得到

    ​                                                            ![](https://img-blog.csdnimg.cn/20190330170736248.png)

* 依次扫描数据集中的每一条记录，提取每条记录中所有可能的3-项集。假设现在有一条记录为{1，3，5，6}，那么其可提取的3-项集为{1,3,5}，{1,3,6}，{1,5,6,}，{3,5,6} ，那么将每一个3-项放在第一步建立的Hash树中寻找对应的桶，然后与桶中的3-项候选频繁项集比较，并为相同的候选项集增加计数。     

    ​                                        ![](https://img-blog.csdnimg.cn/20190401154547209.png)

      经过以上的两步，就完成了所有3-项候选项集的支持度计数。需要强调的是，对于$k$项候选频繁项集的计数，使用Hash函数$h(p)=p \: \:mod \: \:k$来建立于$k$项候选频繁项集的Hash树，然后扫描数据集，提取每一条记录中所有的$k$项集，放入hash树中，对$k$项候选频繁项集进行支持度计数。

### 2.2 FP增长算法

  Apriori算法中每次从![k](https://private.codecogs.com/gif.latex?k)-项候选频繁项集中找出频繁项集时，都需要扫描一次数据集，这在数据集及项集较大时是比较大的负担，FP增长算法通过采用特殊的数据集存储结构——FP树，找出全部的频繁项集只需要扫描数据集两次，能有效的减少计算过程。

在[Apriori算法原理总结](http://www.cnblogs.com/pinard/p/6293298.html)中，我们对Apriori算法的原理做了总结。作为一个挖掘频繁项集的算法，Apriori算法需要多次扫描数据，I/O是很大的瓶颈。为了解决这个问题，FP Tree算法（也称FP Growth算法）采用了一些技巧，无论多少数据，只需要扫描两次数据集，因此提高了算法运行的效率。下面我们就对FP Tree算法做一个总结。

#### 1. FP Tree数据结构

为了减少I/O次数，FP Tree算法引入了一些数据结构来临时存储数据。这个数据结构包括三部分，如下图所示：

![img](https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170119165628718-395589856.png)

第一部分是一个==项头表==。里面记录了所有的1项频繁集出现的次数，按照次数降序排列。比如上图中B在所有10组数据中出现了8次，因此排在第一位，这部分好理解。第二部分是==FP Tree==，它将我们的原始数据集映射到了内存中的一颗FP树，这个FP树比较难理解，它是怎么建立的呢？这个我们后面再讲。第三部分是==节点链表==。所有项头表里的1项频繁集都是一个节点链表的头，它依次指向FP树中该1项频繁集出现的位置。这样做主要是方便项头表和FP Tree之间的联系查找和更新，也好理解。

下面我们讲项头表和FP树的建立过程。

#### 2. 项头表的建立

FP树的建立需要首先依赖项头表的建立。首先我们看看怎么建立项头表。

- 统计原始事务集中各元素项出现的频率

    我们第一次扫描数据，得到所有频繁一项集的的计数。

- 支持度过滤

    删除支持度低于阈值的项，

- 排序

    将1项频繁集放入项头表，并==按照支持度降序排列==。

接着第二次也是最后一次扫描数据，将读到的原始数据剔除非频繁1项集，并按照支持度降序排列。

上面这段话很抽象，我们用下面这个例子来具体讲解。我们有10条数据，首先第一次扫描数据并对1项集计数，我们发现F，O，I，L，J，P，M, N都只出现一次，支持度低于20%的阈值，因此他们不会出现在下面的项头表中。剩下的A,C,E,G,B,D,F按照支持度的大小降序排列，组成了我们的项头表。

接着我们第二次扫描数据，对于每条数据剔除非频繁1项集，并按照支持度降序排列。比如数据项ABCEFO，里面O是非频繁1项集，因此被剔除，只剩下了ABCEF。按照支持度的顺序排序，它变成了ACEBF。其他的数据项以此类推。为什么要将原始数据集里的频繁1项数据项进行排序呢？这是为了我们后面的FP树的建立时，可以尽可能的共用祖先节点。

通过两次扫描，项头表已经建立，排序后的数据集也已经得到了，下面我们再看看怎么建立FP树。

##### 排序好的数据会去掉非频繁1项集

![img](https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170119161846125-505903867.png)

#### 3. FP Tree的建立

　　有了项头表和排序后的数据集，我们就可以开始FP树的建立了。开始时FP树没有数据，建立FP树时我们一条条的读入排序后的数据集，插入FP树，插入时按照排序后的顺序，插入FP树中，排序靠前的节点是祖先节点，而靠后的是子孙节点。==如果有共用的祖先，则对应的公用祖先节点计数加1==。插入后，如果有新节点出现，则项头表对应的节点会通过节点链表链接上新节点。直到所有的数据都插入到FP树后，FP树的建立完成。

​	似乎也很抽象，我们还是用第二节的例子来描述。

　　首先，我们插入第一条数据ACEBF，如下图所示。此时FP树没有节点，因此ACEBF是一个独立的路径，所有节点计数为1, 项头表通过节点链表链接上对应的新增节点。

![img](https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170119163935296-1386696266.png)

　　　　接着我们插入数据ACG，如下图所示。由于ACG和现有的FP树可以有共有的祖先节点序列AC，因此只需要增加一个新节点G，将新节点G的计数记为1。同时A和C的计数加1成为2。当然，对应的G节点的节点链表要更新

![img](https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170119164235343-504556889.png)

　　　　同样的办法可以更新后面8条数据，如下8张图。由于原理类似，这里就不多文字讲解了，大家可以自己去尝试插入并进行理解对比。相信如果大家自己可以独立的插入这10条数据，那么FP树建立的过程就没有什么难度了。

![img](https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170119165253265-145701384.png)

 

![img](https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170119165307640-1886233741.png)

 

![img](https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170119165315890-467841267.png)

 

![img](https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170119165326484-481251658.png)

 

![img](https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170119165335968-745891027.png)

 

![img](https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170119165346437-1176754608.png)

 

![img](https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170119165356531-138078582.png)

 

![img](https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170119165427593-1237891371.png)

 

 

#### 4. FP Tree的挖掘

　　我们辛辛苦苦，终于把FP树建立起来了，那么怎么去挖掘频繁项集呢？看着这个FP树，似乎还是不知道怎么下手。下面我们讲如何从FP树里挖掘频繁项集。得到了FP树和项头表以及节点链表，我们首先要从项头表的底部项依次向上挖掘。对于项头表对应于FP树的每一项，我们要找到它的条件模式基。所谓条件模式基是以我们要挖掘的节点作为叶子节点所对应的FP子树。得到这个FP子树，我们将子树中每个节点的的计数设置为叶子节点的计数，并删除计数低于支持度的节点。从这个条件模式基，我们就可以递归挖掘得到频繁项集了。

　　实在太抽象了，之前我看到这也是一团雾水。还是以上面的例子来讲解。我们看看先从最底下的F节点开始，我们先来寻找F节点的条件模式基，由于F在FP树中只有一个节点，因此候选就只有下图左所示的一条路径，对应{A:8,C:8,E:6,B:2, F:2}。我们接着将所有的祖先节点计数设置为叶子节点的计数，即FP子树变成{A:2,C:2,E:2,B:2, F:2}。一般我们的条件模式基可以不写叶子节点，因此最终的F的条件模式基如下图右所示。

![img](https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170119170723421-1812925376.png)

　　通过它，我们很容易得到F的频繁2项集为{A:2,F:2}, {C:2,F:2}, {E:2,F:2}, {B:2,F:2}。递归合并二项集，得到频繁三项集为{A:2,C:2,F:2}，{A:2,E:2,F:2},...还有一些频繁三项集，就不写了。当然一直递归下去，最大的频繁项集为频繁5项集，为{A:2,C:2,E:2,B:2,F:2}

　　F挖掘完了，我们开始挖掘D节点。D节点比F节点复杂一些，因为它有两个叶子节点，因此首先得到的FP子树如下图左。我们接着将所有的祖先节点计数设置为叶子节点的计数，即变成{A:2, C:2,E:1 G:1,D:1, D:1}此时E节点和G节点由于在条件模式基里面的支持度低于阈值，被我们删除，最终在去除低支持度节点并不包括叶子节点后D的条件模式基为{A:2, C:2}。通过它，我们很容易得到F的频繁2项集为{A:2,D:2}, {C:2,D:2}。递归合并二项集，得到频繁三项集为{A:2,C:2,D:2}。D对应的最大的频繁项集为频繁3项集。

![img](https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170119171924093-1331841220.png)

　　　　同样的方法可以得到B的条件模式基如下图右边，递归挖掘到B的最大频繁项集为频繁4项集{A:2, C:2, E:2,B:2}。

![img](https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170119205610265-1248946944.png)

　　　　继续挖掘G的频繁项集，挖掘到的G的条件模式基如下图右边，递归挖掘到G的最大频繁项集为频繁4项集{A:5, C:5, E:4,G:4}。

![img](https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170119205839703-739252492.png)

　　　　E的条件模式基如下图右边，递归挖掘到E的最大频繁项集为频繁3项集{A:6, C:6, E:6}。

![img](https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170119210046375-59327550.png)

　　　　C的条件模式基如下图右边，递归挖掘到C的最大频繁项集为频繁2项集{A:8, C:8}。

![img](https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170119210254812-1959388744.png)

　　　　至于A，由于它的条件模式基为空，因此可以不用去挖掘了。

　　　　至此我们得到了所有的频繁项集，如果我们只是要最大的频繁K项集，从上面的分析可以看到，最大的频繁项集为5项集。包括{A:2, C:2, E:2,B:2,F:2}。

　　　　通过上面的流程，相信大家对FP Tree的挖掘频繁项集的过程也很熟悉了。

#### 5. FP Tree算法归纳

　　　　这里我们对FP Tree算法流程做一个归纳。FP Tree算法包括三步：

　　　　1）扫描数据，得到所有频繁一项集的的计数。然后删除支持度低于阈值的项，将1项频繁集放入项头表，并按照支持度降序排列。

　　　　2）扫描数据，将读到的原始数据剔除非频繁1项集，并按照支持度降序排列。

　　　　3）读入排序后的数据集，插入FP树，插入时按照排序后的顺序，插入FP树中，排序靠前的节点是祖先节点，而靠后的是子孙节点。如果有共用的祖先，则对应的公用祖先节点计数加1。插入后，如果有新节点出现，则项头表对应的节点会通过节点链表链接上新节点。直到所有的数据都插入到FP树后，FP树的建立完成。

　　　　4）从项头表的底部项依次向上找到项头表项对应的条件模式基。从条件模式基递归挖掘得到项头表项项的频繁项集。

　　　　5）如果不限制频繁项集的项数，则返回步骤4所有的频繁项集，否则只返回满足项数要求的频繁项集。

#### 6. FP tree算法总结

　　　　FP Tree算法改进了Apriori算法的I/O瓶颈，巧妙的利用了树结构，这让我们想起了BIRCH聚类，BIRCH聚类也是巧妙的利用了树结构来提高算法运行速度。利用内存数据结构以空间换时间是常用的提高算法运行时间瓶颈的办法。

　　　　在实践中，FP Tree算法是可以用于生产环境的关联算法，而Apriori算法则做为先驱，起着关联算法指明灯的作用。除了FP Tree，像GSP，CBA之类的算法都是Apriori派系的。

  优点

- FP-tree 算法只需对事务数据库进二次扫描

-  避免产生的大量候选集．

    

缺点

- 要递归生成条件数据库和条件FP-tree， 所以内存开销大
- 只能用于挖掘单维的布尔关联规则 

**3.规则的生成方法**
-------------

规则是从频繁项集中提取的，也可以说是从最大频繁项集中提取。最大频繁项集指的是包含项最多的频繁项集，从最大频繁项集(可能有多个)中一定可以提取出所有的频繁项集。由于在生成频繁项集阶段，就已经获取了所有的频繁项集的支持度计数，因此通过置信度提取规则时，不再需要扫描数据集。

   在生成频繁项集时，可以依据两条先验规则减少计算量，而在提取关联规则时，只有一条规则可以利用：**如果规则**![X\rightarrow Y](https://private.codecogs.com/gif.latex?X%5Crightarrow%20Y)不满足置信度要求，那么![X-X^{`}\rightarrow Y+X^{`}](https://private.codecogs.com/gif.latex?X-X%5E%7B%60%7D%5Crightarrow%20Y&plus;X%5E%7B%60%7D)也不满足置信度要求，其中![X^{`} ](https://private.codecogs.com/gif.latex?X%5E%7B%60%7D)是![X](https://private.codecogs.com/gif.latex?X)的子集。这条规则可以这样理解，假设置信度阈值为![\alpha](https://private.codecogs.com/gif.latex?%5Calpha)，则有

​                                                                                     ![\frac{S(X,Y)}{S(X)}<\alpha](https://private.codecogs.com/gif.latex?%5Cfrac%7BS%28X%2CY%29%7D%7BS%28X%29%7D%3C%5Calpha)

由于![X^{`} ](https://private.codecogs.com/gif.latex?X%5E%7B%60%7D)是![X](https://private.codecogs.com/gif.latex?X)的子集，因此![X^{`} ](https://private.codecogs.com/gif.latex?X%5E%7B%60%7D)的支持度一定不小于![X](https://private.codecogs.com/gif.latex?X)，假设**![X^{`}=X+k ](https://private.codecogs.com/gif.latex?X%5E%7B%60%7D%3DX&plus;k)**，则有

​                                                            ![\frac{S(X-X^{`},Y+X^{`})}{S(X-X^{`})}=\frac{S(X,Y)}{S(X)+k}<\frac{S(X,Y)}{S(X)}<\alpha](https://private.codecogs.com/gif.latex?%5Cfrac%7BS%28X-X%5E%7B%60%7D%2CY&plus;X%5E%7B%60%7D%29%7D%7BS%28X-X%5E%7B%60%7D%29%7D%3D%5Cfrac%7BS%28X%2CY%29%7D%7BS%28X%29&plus;k%7D%3C%5Cfrac%7BS%28X%2CY%29%7D%7BS%28X%29%7D%3C%5Calpha)

  基于该规则，可以采用如下的方式从最大频繁项集中提取规则：

  <1>找出后件只有一个项的所有满足置信度要求的规则。对于那些后件只有一项(假设为![a](https://private.codecogs.com/gif.latex?a))、不满足置信度要求的规则，可以直接剔除掉所有后件中包含![a](https://private.codecogs.com/gif.latex?a)的规则，例如

​                 ![](https://img-blog.csdnimg.cn/20190401160316815.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h1Z3VvemhpZW5ncg==,size_16,color_FFFFFF,t_70)

  <2>通过合并两个规则后件生成新的候选规则，然后判断其是否满足置信度要求，同样的，剔除掉那些不满足置信度要求的候选规则，以及这些规则中后件的超集对应的规则。例如，通过合并 ![abd\rightarrow c](https://private.codecogs.com/gif.latex?abd%5Crightarrow%20c) 与 ![abc\rightarrow d](https://private.codecogs.com/gif.latex?abc%5Crightarrow%20d) 得到新的候选规则 ![ab\rightarrow cd](https://private.codecogs.com/gif.latex?ab%5Crightarrow%20cd)，如果该规则不满足置信度要求，那么后件中包含![cd](https://private.codecogs.com/gif.latex?cd)的候选规则也均不满足要求，例如![a\rightarrow bcd](https://private.codecogs.com/gif.latex?a%5Crightarrow%20bcd)。

  <3>按照前两步的方式，通过逐步合并规则后件生成候选规则，然后对这些候选规则进行筛选，得到满足置信度要求的规则。

**4.小结**
--------

   通过以上内容的介绍，我们大致知道了在挖掘购物数据项集中的关联规则时，需要考虑的问题，也就是文中一开始提出的两个问题，也知道了可以用来解决这两个问题的方法，包括使用支持度和置信度筛选频繁项集和规则，使用Apriori算法或者FP 增长算法获取频繁项集和规则。

  用支持度与置信度来评估关联规则并不是唯一选择，甚至在一些情况下它们并不适合使用，在系列的下一篇中，我们将讨论一下关联规则的评估方法。