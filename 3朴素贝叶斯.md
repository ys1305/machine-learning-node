贝叶斯分类器
======

一、贝叶斯定理
-------

### 1.1 贝叶斯定理

1. 设 $\mathbb S$ 为试验 E 的样本空间；$ B_1,B_2,\cdots,B_n$ 为 E 的一组事件。若 ：

    *   $B_i \bigcap B_j=\phi,i \ne j,i,j=1,2,\cdots,n$
    *   $B_1 \bigcup B_2 \bigcup \cdots \bigcup B_n=\mathbb S$

    则称 $B_1,B_2,\cdots,B_n$ 为样本空间$ \mathbb S$ 的一个划分。

2. 如果 $B_1,B_2,\cdots,B_n$ 为样本空间 $\mathbb S $的一个划分，则对于每次试验，事件$ B_1,B_2,\cdots,B_n$ 中有且仅有一个事件发生。

3. 全概率公式 ：设试验 E 的样本空间为$ \mathbb S ， A $为$ E$ 的事件，$ B_1,B_2,\cdots,B_n $为样本空间 $\mathbb S $的一个划分，且 $p(B_i) \ge 0(i=1,2,\cdots,n) $。则有：

    $p(A)=p(A\mid B_1)p(B_1)+p(A\mid B_2)p(B_2)+\cdots+p(A\mid B_n)p(B_n)=\sum_{j=1}^{n}p(A\mid B_j)p(B_j)$

4. 贝叶斯定理 ：设试验$ E$ 的的样本空间为$ \mathbb S， A $为$ E $的事件，$ B_1,B_2,\cdots,B_n $为样本空间$ \mathbb S $的一个划分，且 $p(A) \gt 0,p(B_i) \ge 0(i=1,2,\cdots,n) $，则有：$p(B_i\mid A)=\frac{p(A\mid B_i)p(B_i)}{\sum_{j=1}^{n}p(A\mid B_j)p(B_j)} 。$


### 1.2 先验概率、后验概率

1. 先验概率：根据以往经验和分析得到的概率。

    后验概率：根据已经发生的事件来分析得到的概率。

2. 例：假设山洞中有熊出现的事件为$ Y$ ，山洞中传来一阵熊吼的事件为$ X$ 。

    *   山洞中有熊的概率为$ p(Y) $。它是先验概率，根据以往的数据分析或者经验得到的概率。
    *   听到熊吼之后认为山洞中有熊的概率为 $p(Y\mid X) $。它是后验概率，得到本次试验的信息从而重新修正的概率。

二、朴素贝叶斯法-多项式朴素贝叶斯
--------

1. 朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法。

    对给定的训练集：

    *   首先基于特征条件独立假设学习输入、输出的联合概率分布。
    *   然后基于此模型，对给定的输入 $\mathbf {\vec x} $，利用贝叶斯定理求出后验概率最大的输出 $y$ 。

2. ==朴素贝叶斯法不是贝叶斯估计，贝叶斯估计是最大后验估计==。

3. ==朴素贝叶斯是一个不建模的算法==。我们认为，训练集和测试集都来自于同一个不可获得的大样本下，并且这个大样本下的各种属性所表现出来的规律应当是一致的，因此训练集上计算出来的各种概率，可以直接放到测试集上来使用。即便不建模，也可以完成分类 .


### 2.1 原理

1. 设输入空间$ \mathcal X \subseteq \mathbb R^{n} $为$ n $维向量的集合 ，输出空间为类标记集合$\mathcal Y =\{ c_1,c_2,\cdots,c_k\} $。

    令$ \mathbf{\vec x} =(x_1,x_2,\cdots,x_n)^T $为定义在$ \mathcal X $上的随机向量，$y$ 为定义在$ \mathcal Y $上的随机变量。

    令$ p(\mathbf{\vec x},y)$ 为 $\mathbf{\vec x} $和$ y $的联合概率分布，假设训练数据集$ \mathbb D=\{(\mathbf {\vec x}_1,\tilde y_1),(\mathbf {\vec x}_2,\tilde y_2),\cdots,(\mathbf {\vec x}_N,\tilde y_N)\}$ 由 $p(\mathbf{\vec x},y) $独立同分布产生。

    朴素贝叶斯法通过训练数据集学习联合概率分布$ p(\mathbf{\vec x},y)$。具体的学习下列概率分布：

    *   先验概率分布：$ p(y) $。
    *   条件概率分布：$ p( \mathbf {\vec x}\mid y)=p(x_1,x_2,\cdots,x_n \mid y) $。

2. 朴素贝叶斯法对条件概率做了特征独立性假设：$p( \mathbf{\vec x}\mid y )=p(x_1,x_2,\cdots,x_n\mid y )=\prod_{j=1}^{n}p(x_j\mid y ) $。

    *   这意味着在分类确定的条件下，用于分类的特征是条件独立的。
    *   该假设使得朴素贝叶斯法变得简单，但是可能牺牲一定的分类准确率。

3. 根据贝叶斯定理：

    $$
    p(y\mid \mathbf {\vec x})=\frac{p( \mathbf {\vec x}\mid y)p(y)}{\sum_{y^\prime} p( \mathbf {\vec x}\mid y^\prime)p(y^\prime)}
    $$
    考虑分类特征的条件独立假设有：

    $$
    p(y\mid \mathbf {\vec x})=\frac{p(y)\prod_{i=1}^{n}p(x_i\mid y)}{\sum_{y^\prime} p( \mathbf {\vec x}\mid y^\prime)p(y^\prime)}
    $$
    则朴素贝叶斯分类器表示为：

    $$
    f(\mathbf {\vec x})=\arg \max_{y \in \mathcal Y}\frac{p(y)\prod_{i=1}^{n}p(x_i\mid y)}{\sum_{y^\prime} p( \mathbf {\vec x}\mid y^\prime)p(y^\prime)}
    $$
    由于上式的分母 $p(\mathbf {\vec x})$ 与$ y $的取值无关，则分类器重写为：$f(\mathbf {\vec x})=\arg \max_{y \in \mathcal Y} p(y)\prod_{i=1}^{n}p(x_i\mid y) $。

    ![img](https://ask.qcloudimg.com/http-save/yehe-1168707/xbt1zs6wl5.png?imageView2/2/w/1620)

![img](https://ask.qcloudimg.com/http-save/yehe-1168707/siejgxyzoa.png?imageView2/2/w/1620)



$j$为特征,$c_k$为类别





### 2.2 期望风险最小化

1. 朴素贝叶斯分类器是后验概率最大化，等价于期望风险最小化。

2. 令损失函数为：

    $$
    L(y,f(\mathbf{\vec x}))= \begin{cases} 1, & y \ne f(\mathbf{\vec x}) \\ 0, & y=f(\mathbf{\vec x}) \end{cases} \\ R_{exp}(f)=\mathbb E[L(y,f(\mathbf{\vec x}))]=\sum_{\mathbf {\vec x} \in \mathcal X}\sum_{y \in \mathcal Y}L(y,f(\mathbf {\vec x}))p(\mathbf {\vec x}, y)
    $$

3. 根据$ p(\mathbf{\vec x},y)=p(\mathbf{\vec x})p(y \mid \mathbf{\vec x})$ 有：

    $$
    R_{exp}(f)=\mathbb E[L(y,f(\mathbf{\vec x}))]=\sum_{\mathbf {\vec x} \in \mathcal X}\sum_{y \in \mathcal Y}L(y,f(\mathbf {\vec x}))p(\mathbf {\vec x}, y) =\mathbb E_X[\sum_{y\in \mathcal Y} L(y,f(\mathbf {\vec x}))p(y\mid \mathbf {\vec x})]
    $$
    为了使得期望风险最小化，只需要对$ \mathbb E_X $中的元素极小化。

    令 $\hat y=f(\mathbf{\vec x}) $，则有：

    $$
    \arg\min_{\hat y} \sum_{y\in \mathcal Y}L(y,\hat y) p(y\mid \mathbf{\vec x})=\arg\min_{\hat y} \sum_{y\in \mathcal Y}p(y\ne \hat y\mid \mathbf{\vec x} )\\ =\arg\min_{\hat y}( 1-p( \hat y\mid \mathbf{\vec x} )) = \arg\max_{\hat y}p( \hat y\mid \mathbf{\vec x} )
    $$
    即：==期望风险最小化，等价于后验概率最大化==。

    $y$为随机变量,可以取遍所有的类别,$\hat {y}$只能取类别中的一个

    


### 2.3 算法-极大似然估计

1. 在朴素贝叶斯法中，学习意味着估计概率：$p(y)， p( x_i\mid y)$ 。

2. 可以用极大似然估计相应概率。

    * 先验概率$ p(y)$ 的==极大似然估==计为：$p(y=c_k)=\frac {1}{N} \sum_{i=1}^{N}I(\tilde y_i=c_k)$

    * 设第$ j $个特征$ x_j $可能的取值为$ \{a_{j,1},a_{j,2},\cdots,a_{j,s_j}\}，$则条件概率$ p(x_j=a_{j,l}\mid y=c_k)$ 的极大似然估计为：

        $$
        p(x_j=a_{j,l}\mid y=c_k)=\frac{\sum_{i =1}^{N}I(x_{i ,j}=a_{j,l},\tilde y_{i }=c_k)}{\sum_{i =1}^{N}I(\tilde y_{i }=c_k)}\\ j=1,2,\cdots,n; \;l=1,2,\cdots,s_j; \;k=1,2,\cdots,K
        $$
        其中：$I $为示性函数，$ x_{i ,j}$ 表示第$ i $个样本的第$ j $个特征。

3. 朴素贝叶斯算法 ：

    * 输入 ：

        * 训练集$ \mathbb D=\{(\mathbf {\vec x}_1,\tilde y_1),(\mathbf {\vec x}_2,\tilde y_2),\cdots,(\mathbf {\vec x}_N,\tilde y_N)\}$ 。

            $\mathbf {\vec x}_i=(x_{i,1},x_ {i,2},\cdots,x_ {i,n})^{T}, x_{i,j} $为第$ i $个样本的第 $j $个特征。其中 $x_{i,j} \in \{a_{j,1},a_{j,2},\cdots,a_{j,s_j}\}， a_{j , l}$为第 $j $个特征可能取到的第$ l $个值。

        * 实例$ \mathbf {\vec x} $。

    * 输出 ：实例$ \mathbf {\vec x} $的分类

    * 算法步骤：

        *   计算先验概率以及条件概率：

        $$
        p(y=c_k)=\frac {1}{N} \sum_{i=1}^{N}I(\tilde y_i=c_k),k=1,2,\cdots,K\\ p(x_j=a_{j,l}\mid y=c_k)=\frac{\sum_{i =1}^{N}I(x_{i ,j}=a_{j,l},\tilde y_{i }=c_k)}{\sum_{i =1}^{N}I(\tilde y_{i }=c_k)}\\ j=1,2,\cdots,n; \;l=1,2,\cdots,s_j; \;k=1,2,\cdots,K
        $$

        

        * 对于给定的实例 ，$\mathbf {\vec x}=(x_1，x_2,\cdots,x_n)^{T}$，计算：$p(y=c_k)\prod_{j=1}^{n}p( x_j\mid y=c_k) $。

        * 确定实例 $\mathbf {\vec x}$ 的分类：$\hat y= \arg\max_{c_k}p(y=c_k)\prod_{j=1}^{n}p( x_j\mid y=c_k) $。

            ![img](https://ask.qcloudimg.com/http-save/yehe-1168707/imcqbbz8bn.png?imageView2/2/w/1620)

    

#### 李航书中的例子

![img](https://ask.qcloudimg.com/http-save/yehe-1168707/m5n52iu2bg.png?imageView2/2/w/1620)



![img](https://ask.qcloudimg.com/http-save/yehe-1168707/d675dhc0cl.png?imageView2/2/w/1620)

### 2.4 贝叶斯估计

1. 在估计概率$ p( x_i\mid y) $的过程中，分母 $\sum_{i =1}^{N}I(\tilde y_{i }=c_k) $可能为 0 。这是==由于训练样本太少才导致$ c_k$ 的样本数为 0== 。而真实的分布中，$ c_k$ 的样本并不为 0 。

    解决的方案是采用==贝叶斯估计==（最大后验估计）。

    满足的先验分布为Dirichlet分布

2. 假设第$ j $个特征 $x_j$ 可能的取值为 $\{a_{j,1},a_{j,2},\cdots,a_{j,s_j}\}$ ，贝叶斯估计假设在每个取值上都有一个先验的计数$ \lambda $。即：

    $$
    p_{\lambda}(x_j=a_{j,l}\mid y=c_k)=\frac{\sum_{i =1}^{N}I(x_{i ,j}=a_{j,l},\tilde y_{i }=c_k)+\lambda}{\sum_{i=1}^{N}I(\tilde y_i=c_k)+s_j\lambda}\\ j=1,2,\cdots,n; \;l=1,2,\cdots,s_j; \;k=1,2,\cdots,K
    $$
    它等价于在 $x_j $的各个取值的频数上赋予了一个正数$ \lambda$。

    $s_j$为第 $j $个特征可能取值得数量。

    若$ c_k $的样本数为0，则它假设特征$ x_j $每个取值的概率为$ \frac{1}{s_j}$，即等可能的。

3. 采用贝叶斯估计后，$p(y) $的贝叶斯估计调整为:

    $$
    p_\lambda(y=c_k)=\frac{\sum_{i=1}^{N}I(\tilde y_i=c_k)+\lambda}{N+K\lambda}
    $$

*   当$ \lambda=0$ 时，为极大似然估计,当$ \lambda=1 $时，为拉普拉斯平滑
*   若 $c_k$ 的样本数为 0，则假设赋予它一个非零的概率$ \frac{\lambda}{N+K\lambda}$ 。



![img](https://ask.qcloudimg.com/http-save/yehe-1168707/6sayjilhic.png?imageView2/2/w/1620)

#### 贝叶斯估计原理

##### 基于多项分布与Dirichlet先验分布的贝叶斯估计

来自 <https://lucius-yu.github.io/docs/probability/BaysianEstimation/>

对于n次实验,试验结果可能有k种取值,记$x_i$为第i种取值发生的次数, 采用超参为$\alpha=(\alpha_1,…,\alpha_k)$的Dirichlet分布为参数$\theta = (\theta_1, … \theta_k)$先验概率分布, 其后验概率如下
$$
p(\theta \vert D) = \frac{\Gamma(\sum_{i=1}^k \alpha_i + n)}{\prod_{i=1}^k \Gamma(\alpha_i+x_i)} \prod_{i=1}^k \theta_i^{\alpha_i+x_i-1} = \frac{1}{Beta(\alpha^\prime)} \prod_{i=1}^k \theta_i^{\alpha_i^\prime-1}
$$
其中$\alpha^\prime = (\alpha_1^\prime,…, \alpha_k^\prime) = (\alpha_1+x_1, …, \alpha_k+x_k)$

推导过程类似于二项分布时,用Beta分布做先验分布求后验概率. 此处略去.





三、半朴素贝叶斯分类器
-----------

1. 朴素贝叶斯法对条件概率做了特征的独立性假设：$p( \mathbf{\vec x}\mid y )=p(x_1,x_2,\cdots,x_n\mid y )=\prod_{j=1}^{n}p(x_j\mid y ) $。

    但是现实任务中这个假设有时候很难成立。若对特征独立性假设进行一定程度上的放松，这就是半朴素贝叶斯分类器`semi-naive Bayes classifiers` 。

2. 半朴素贝叶斯分类器原理：适当考虑一部分特征之间的相互依赖信息，从而既不需要进行完全联合概率计算，又不至于彻底忽略了比较强的特征依赖关系。


### 3.1 独依赖估计 OED

1. 独依赖估计`One-Dependent Estimator:OED`是半朴素贝叶斯分类器最常用的一种策略。它假设每个特征在类别之外最多依赖于一个其他特征，即：

    $$
    p( \mathbf{\vec x}\mid y)=p(x_1,x_2,\cdots, x_n\mid y) =\prod_{j=1}^{n}p(x_j\mid y,x_j^P)
    $$
    其中 $x_j^P $为特征$ x_j$ 所依赖的特征，称作的$ x_j 父$特征。

2. 如果父属性已知，那么可以用贝叶斯估计来估计概率值 $p(x_j\mid y,x_j^P) $。现在的问题是：如何确定每个特征的父特征？

    不同的做法产生不同的独依赖分类器。


#### 3.1.1 SPODE

1. 最简单的做法是：假设所有的特征都依赖于同一个特征，该特征称作超父。然后通过交叉验证等模型选择方法来确定超父特征。这就是`SPODE:Super-Parent ODE`方法。

    假设节点 `Y` 代表输出变量$ y $，节点 `Xj` 代表属性 $x_j$ 。下图给出了超父特征为$ x_1$ 时的 `SPODE` 。

    ![img](http://www.huaxiaozhuan.com/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/imgs/bayesian/spode.png)


#### 3.1.2 TAN

1. `TAN:Tree Augmented naive Bayes`是在最大带权生成树算法基础上，通过下列步骤将特征之间依赖关系简化为如下图所示的树型结构：

    * 计算任意两个特征之间的条件互信息。记第 $i $个特征 $x_i $代表的结点为 $\mathbf X_i$，标记代表的节点为 $\mathbf{Y} $则有:

        $$
        I(\mathbf X_i,\mathbf X_j\mid \mathbf Y)=\sum_y\sum_{x_i}\sum_{x_j} p(x_i,x_j\mid y)\log \frac{p(x_i,x_j\mid y)}{p(x_i\mid y)p(x_j\mid y)}
        $$
        如果两个特征 $x_i,x_j$ 相互条件独立，则 $p(x_i,x_j\mid y)=p(x_i\mid y)p(x_j\mid y) $。则有条件互信息 $I(\mathbf X_i,\mathbf X_j\mid \mathbf Y)=0$，则在图中这两个特征代表的结点没有边相连。

    * 以特征为结点构建完全图，任意两个结点之间边的权重设为条件互信息$ I(\mathbf X_i, \mathbf X_j\mid \mathbf Y) $。

    * 构建此完全图的最大带权生成树，挑选根结点（下图中根节点为节点$ \mathbf X_1)$，将边置为有向边。

    * 加入类别结点 $\mathbf Y $，增加 $\mathbf Y$ 到每个特征的有向边。因为所有的条件概率都是以 y 为条件的。 

        ![img](http://www.huaxiaozhuan.com/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/imgs/bayesian/tan.png)

四、其它讨论
------

1.  朴素贝叶斯分类器的优点：

    *   性能相当好，它速度快，可以避免维度灾难。
    *   支持大规模数据的并行学习，且天然的支持增量学习。
2.  朴素贝叶斯分类器的缺点：

    *   无法给出分类概率，因此难以应用于需要分类概率的场景。





## 五 伯努利朴素贝叶斯

与多项式模型一样，伯努利模型适用于离散特征的情况，所不同的是，伯努利模型中每个特征的取值只能是1和0(以文本分类为例，某个单词在文档中出现过，则其特征值为1，否则为0).

伯努利模型中，条件概率$P(x_i|y_k)$的计算方式是：

当特征值$x_i$为1时，$P(x_i|y_k)=P(x_i=1|y_k)$；

当特征值$x_i$为0时，$P(x_i|y_k)=1−P(x_i=1|y_k)$；

伯努利模型和多项式模型是一致的，BernoulliNB需要比MultinomialNB多定义一个二值化的方法，该方法会接受一个阈值并将输入的特征二值化（1，0）。当然也可以直接采用MultinomialNB，但需要预先将输入的特征二值化。

#### 文本分类的应用

**1、基本定义：**

分类是把一个事物分到某个类别中。一个事物具有很多属性，把它的众多属性看作一个向量，即x=(x1,x2,x3,…,xn)，用x这个向量来代表这个事物，x的集合记为X，称为属性集。类别也有很多种，用集合C={c1,c2,…cm}表示。一般X和C的关系是不确定的，可以将X和C看作是随机变量，P(C|X)称为C的后验概率，与之相对的，P(C)称为C的先验概率。

根据贝叶斯公式，后验概率P(C|X)=P(X|C)P(C)/P(X)，但在比较不同C值的后验概率时，分母P(X)总是常数，忽略掉，后验概率P(C|X)=P(X|C)P(C)，先验概率P(C)可以通过计算训练集中属于每一个类的训练样本所占的比例，容易估计，对类条件概率P(X|C)的估计，这里我只说朴素贝叶斯分类器方法，因为朴素贝叶斯假设事物属性之间相互条件独立，P(X|C)=∏P(xi|ci)。

**2、文本分类过程**

例如文档：Good good study Day day up可以用一个文本特征向量来表示，x=(Good, good, study, Day, day , up)。在文本分类中，假设我们有一个文档d∈X，类别c又称为标签。我们把一堆打了标签的文档集合<d,c>作为训练样本，<d,c>∈X×C。例如：<d,c>={Beijing joins the World Trade Organization, China}对于这个只有一句话的文档，我们把它归类到 China，即打上china标签。

朴素贝叶斯分类器是一种有监督学习，常见有两种模型，多项式模型(multinomial model)即为词频型和伯努利模型(Bernoulli model)即文档型。二者的计算粒度不一样，多项式模型以单词为粒度，伯努利模型以文件为粒度，因此二者的先验概率和类条件概率的计算方法都不同。计算后验概率时，对于一个文档d，多项式模型中，只有在d中出现过的单词，才会参与后验概率计算，伯努利模型中，没有在d中出现，但是在全局单词表中出现的单词，也会参与计算，不过是作为“反方”参与的。这里暂不考虑特征抽取、为避免消除测试文档时类条件概率中有为0现象而做的取对数等问题。

#### **2.1多项式模型**

1）基本原理

在多项式模型中， 设某文档d=(t1,t2,…,tk)，tk是该文档中出现过的单词，允许重复，则

先验概率P(c)= 类c下单词总数/整个训练样本的单词总数

类条件概率P(tk|c)=(类c下单词tk在各个文档中出现过的次数之和+1)/(类c下单词总数+|V|)

V是训练样本的单词表（即抽取单词，单词出现多次，只算一个），|V|则表示训练样本包含多少种单词。 P(tk|c)可以看作是单词tk在证明d属于类c上提供了多大的证据，而P(c)则可以认为是类别c在整体上占多大比例(有多大可能性)。

2）举例

给定一组分好类的文本训练数据，如下：

| docId | doc                      | 类别In c=China? |
| ----- | ------------------------ | --------------- |
| 1     | Chinese Beijing Chinese  | yes             |
| 2     | Chinese Chinese Shanghai | yes             |
| 3     | Chinese Macao            | yes             |
| 4     | Tokyo Japan Chinese      | no              |

给定一个新样本Chinese Chinese Chinese Tokyo Japan，对其进行分类。该文本用属性向量表示为d=(Chinese, Chinese, Chinese, Tokyo, Japan)，类别集合为Y={yes, no}。

类yes下总共有8个单词，类no下总共有3个单词，训练样本单词总数为11，因此P(yes)=8/11, P(no)=3/11。类条件概率计算如下：

P(Chinese | yes)=(5+1)/(8+6)=6/14=3/7

P(Japan | yes)=P(Tokyo | yes)= (0+1)/(8+6)=1/14

P(Chinese|no)=(1+1)/(3+6)=2/9

P(Japan|no)=P(Tokyo| no) =(1+1)/(3+6)=2/9

分母中的8，是指yes类别下textc的长度，也即训练样本的单词总数，6是指训练样本有Chinese,Beijing,Shanghai, Macao, Tokyo, Japan 共6个单词，3是指no类下共有3个单词。

有了以上类条件概率，开始计算后验概率：

P(yes | d)=(3/7)^3×1/14×1/14×8/11=108/184877≈0.00058417

P(no | d)= (2/9)^3×2/9×2/9×3/11=32/216513≈0.00014780

比较大小，即可知道这个文档属于类别china。

#### **2.2伯努利模型**

1）基本原理

P(c)= 类c下文件总数/整个训练样本的文件总数

P(tk|c)=(类c下包含单词tk的文件数+1)/(类c下单词总数+2)

2）举例

使用前面例子中的数据，模型换成伯努利模型。

类yes下总共有3个文件，类no下有1个文件，训练样本文件总数为11，因此P(yes)=3/4, P(Chinese | yes)=(3+1)/(3+2)=4/5，条件概率如下：

P(Japan | yes)=P(Tokyo | yes)=(0+1)/(3+2)=1/5

P(Beijing | yes)= P(Macao|yes)= P(Shanghai |yes)=(1+1)/(3+2)=2/5

P(Chinese|no)=(1+1)/(1+2)=2/3

P(Japan|no)=P(Tokyo| no) =(1+1)/(1+2)=2/3

P(Beijing| no)= P(Macao| no)= P(Shanghai | no)=(0+1)/(1+2)=1/3

有了以上类条件概率，开始计算后验概率，

P(yes|d)=P(yes)×P(Chinese|yes)×P(Japan|yes)×P(Tokyo|yes)×(1-P(Beijing|yes))×(1-P(Shanghai|yes))×(1-P(Macao|yes))=3/4×4/5×1/5×1/5×(1-2/5) ×(1-2/5)×(1-2/5)=81/15625≈0.005

P(no|d)= 1/4×2/3×2/3×2/3×(1-1/3)×(1-1/3)×(1-1/3)=16/729≈0.022

因此，这个文档不属于类别china。

**后记：**文本分类是作为离散型数据的，以前糊涂是把连续型与离散型弄混一块了，朴素贝叶斯用于很多方面，数据就会有连续和离散的，连续型时可用正态分布，还可用区间，将数据的各属性分成几个区间段进行概率计算，测试时看其属性的值在哪个区间就用哪个条件概率。再有TF、TDIDF，这些只是描述事物属性时的不同计算方法，例如文本分类时，可以用单词在本文档中出现的次数描述一个文档，可以用出现还是没出现即0和1来描述，还可以用单词在本类文档中出现的次数与这个单词在剩余类出现的次数（降低此属性对某类的重要性）相结合来表述。







## 六 高斯朴素贝叶斯

如果样本特征的分布大部分是连续值，使用GaussianNB会比较好 

当特征是连续变量的时候，运用多项式模型就会导致很多$P(x_i|y_k)=0$（不做平滑的情况下），此时即使做平滑，所得到的条件概率也难以描述真实情况。所以处理连续的特征变量，应该采用高斯模型。

### 高斯模型假设每一维特征都服从正态分布：

$$
P(x_{i}|y_{k})=\frac{1}{\sqrt{2\pi\sigma_{y_{k},i}^{2}}}e^{-\frac{(x_{i}-\mu_{y_{k},i})^{2}}{2  \sigma_{y_{k},i}^{2}}}
$$

$μ_{y_k,i}$表示类别为$y_k$的样本中，第$i$维特征的均值。 
$σ^2_{y_k,i}$表示类别为$y_k$的样本中，第$i$维特征的方差。

每个特征之间都是独立的,不能求特征之间的协方差矩阵



```python
class NaiveBayes:
    def __init__(self):
        self.model = None

    # 数学期望
    @staticmethod
    def mean(X):
        return sum(X) / float(len(X))

    # 标准差（方差）
    def stdev(self, X):
        avg = self.mean(X)
        return math.sqrt(sum([pow(x-avg, 2) for x in X]) / float(len(X)))

    # 概率密度函数
    def gaussian_probability(self, x, mean, stdev):
        exponent = math.exp(-(math.pow(x-mean,2)/(2*math.pow(stdev,2))))
        return (1 / (math.sqrt(2*math.pi) * stdev)) * exponent

    # 处理X_train
    def summarize(self, train_data):
        summaries = [(self.mean(i), self.stdev(i)) for i in zip(*train_data)]
        return summaries

    # 计算出每个类别的数学期望和标准差
    def fit(self, X, y):
        labels = list(set(y))
        data = {label:[] for label in labels}
        for f, label in zip(X, y):
            data[label].append(f)
        self.model = {label: self.summarize(value) for label, value in data.items()}
        return 'gaussianNB train done!'

    # 计算概率
    def calculate_probabilities(self, input_data):
        # summaries:{0.0: [(5.0, 0.37),(3.42, 0.40)], 1.0: [(5.8, 0.449),(2.7, 0.27)]}
        # input_data:[1.1, 2.2]
        probabilities = {}
        for label, value in self.model.items():
            probabilities[label] = 1
            for i in range(len(value)):
                mean, stdev = value[i]
                probabilities[label] *= self.gaussian_probability(input_data[i], mean, stdev)
        return probabilities

    # 类别
    def predict(self, X_test):
        # print(self.calculate_probabilities(X_test).items())
        # dict_items([(0.0, 0.46216164346529376), (1.0, 2.917762081359877e-18), (2.0, 9.836470021805279e-31)])
        # print(sorted(self.calculate_probabilities(X_test).items(), key=lambda x: x[-1]))
        # 从小到大排序
        # [(2.0, 1.0446327049122698e-26), (1.0, 1.612020738629649e-18), (0.0, 0.9025722438875156)]

        label = sorted(self.calculate_probabilities(X_test).items(), key=lambda x: x[-1])[-1][0]
        
        return label

    def score(self, X_test, y_test):
        right = 0
        for X, y in zip(X_test, y_test):
            label = self.predict(X)
            if label == y:
                right += 1

        return right / float(len(X_test))
```



##### 菊安的实现方式

```python
def gnb_classify(train,test):
    labels = train.iloc[:,-1].value_counts().index #提取训练集的标签种类
    mean =[] #存放每个类别的均值
    std =[] #存放每个类别的方差
    result = [] #存放测试集的预测结果
    for i in labels:
        item = train.loc[train.iloc[:,-1]==i,:] #分别提取出每一种类别
        m = item.iloc[:,:-1].mean() #当前类别的平均值
        s = np.sum((item.iloc[:,:-1]-m)**2)/(item.shape[0]) #当前类别的方差
        mean.append(m) #将当前类别的平均值追加至列表
        std.append(s) #将当前类别的方差追加至列表
    means = pd.DataFrame(mean,index=labels) #变成DF格式，索引为类标签
    stds = pd.DataFrame(std,index=labels) #变成DF格式，索引为类标签
    for j in range(test.shape[0]):
        iset = test.iloc[j,:-1].tolist() #当前测试实例
        iprob = np.exp(-1*(iset-means)**2/(stds*2))/(np.sqrt(2*np.pi*stds)) #正态分布公式
        # print(iprob.shape)
        # 3,4 :3对应的是三个类别，4对应的是每个样本有四个特征

        # 用log求和
        prob = np.sum(np.log(iprob),axis=1)

        # prob = 1 #初始化当前实例总概率
        # for k in range(test.shape[1]-1): #遍历每个特征
        #     prob *= iprob[k] #特征概率之积即为当前实例概率
        cla = prob.index[np.argmax(prob.values)] #返回最大概率的类别
        result.append(cla)
    test['predict']=result
    acc = (test.iloc[:,-1]==test.iloc[:,-2]).mean() #计算预测准确率
    print(f'模型预测准确率为{acc}')
    return test
```

