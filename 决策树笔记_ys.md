# 决策树

1. 决策树是一种基本的分类与回归方法。这里主要讨论决策树用于分类。

2. 决策树模型是描述对样本进行分类的树形结构。树由结点和有向边组成：

    - 内部结点表示一个特征或者属性。
    - 叶子结点表示一个分类。
    - 有向边代表了一个划分规则。

3. 决策树从根结点到子结点的的有向边代表了一条路径。决策树的路径是互斥并且是完备的。

4. 用决策树分类时，对样本的某个特征进行测试，根据测试结果将样本分配到树的子结点上。此时每个子结点对应该特征的一个取值。

    递归地对样本测试，直到该样本被划分叶结点。最后将样本分配为叶结点所属的类。

5. 决策树的优点：可读性强，分类速度快。

6. 决策树学习通常包括3个步骤：

    - 特征选择。
    - 决策树生成。
    - 决策树剪枝。










| 不纯度                                                       |
| ------------------------------------------------------------ |
| 决策树的每个叶子节点中都会包含一组数据，在这组数据中，如果有某一类标签占有较大的比例，我们就说叶子节点“纯”，分枝分得好。某一类标签占的比例越大，叶子就越纯，不纯度就越低，分枝就越好。如果没有哪一类标签的比例很大，各类标签都相对平均，则说叶子节点”不纯“，分枝不好，不纯度高。 |





# 计算不纯度的三种方法

![1562141043673](assets/1562141043673.png)

![1562141065553](assets/1562141065553.png)





## **决策树最终的优化目标是使得叶节点的总不纯度最低，即对应衡量不纯度的指标最低**



![1562141087052](assets/1562141087052.png)

$N=|D|,N_k=|C_k|$

# ID3算法

李航ppt中有个十分详细的ID3例子

## 基本公式

![1562141113137](assets/1562141113137.png)



- 经验熵称为信息熵

- 经验条件熵称为加权信息熵

对训练数据集（或子集）D，计算其每个特征的信息增益，并比较它们的大小，选择**信息增益最大**的特征

## `ID3` 生成算法：

- 输入：

  - 训练数据集 $D$ 
  - 特征集合 $\bar{A}={A_1,A_2,...,A_n}$ 
  - 特征信息增益阈值 $\varepsilon>0$

- 输出：决策树 

- 算法步骤：

  - 若 $D$中所有样本均属于同一类 $C_k$，则$T$为单结点树，并将$C_k $作为该结点的类标记，算法终止。

  - 若 $\bar{A}=\phi$，则$T$为单结点树，将$D$中样本数最大的类$C_k$ 作为该结点的类标记，算法终止。

  - 否则计算 $g(D,A_j),j=1,2,..,n$，选择信息增益最大的特征  $A_g$：

    - 若 $g(D,A_g)<\varepsilon$，则置T为单结点树，将$D$中样本数最大的类 $C_k$作为该结点的类标记，算法终止 。

    - 若  $g(D,A_j)>=\varepsilon$，则对 $A_g$ 特征的每个可能取值  $a_i,i=1,2,..,n_g$，根据  $A_g=a_i$将  $D$划分为若干个非空子集  。

      将$D_i$中样本数最大的类作为该子集的标记，构建子结点。

  - 对第 $i$个子结点， 以 $D_i$为训练集， 以$\bar{A}-A_g$为特征集，递归地调用前面的步骤来构建子树。

    即$D^{new}=D_i,\bar{A}^{new}=\bar{A}^{old}-A_g$

注：上面的$\bar{A}$为特征的集合，$A_j$为单个的特征;

### 第二次分枝

一个属性只能用一次：当这个属性被当做父节点后，则其子节点就不会再用这个属性了，因为每个子节点的这个属性都是一致的，不能进行划分了.但和其同一层的节点还是可以进行划分的。

青年节点的经验熵的计算如下:

![1562141140368](assets/1562141140368.png)

这是按照年龄进行分裂后的一个子树,所以在这个子树上的年龄属性都是一致的,没有计算年龄属性的必要了。只能用剩余的三个属性进行计算。且其余子树也是用剩下的三个属性进行计算==.每颗子树之间的计算不会影响.==或许所有的子树都会按照收入这个属性进行分裂也是可能的。

下面做的就是将这颗子树当做父节点进行计算.而且经验熵的计算不会乘以384/1025(这棵树的样本为384，总样本数量为1025),其实这个经验熵在上一步计算经验条件熵时已经计算过了。

==第二次分枝时的$H(D)$为第一次分枝时计算年龄的信息增益中的$H(D_1)$==

见下，第一次分枝时计算年龄的信息增益，先计算青年的$H(D_1)$ ,

![1562141162664](assets/1562141162664.png)



计算中年的$H(D_2)=0$,与老年的$H(D_3)=0.9157$，接下来根据上面得到的三个$H(D_i)$计算经验条件熵，也就是图中的平均信息期望$E(年龄) $,得到E后，可以计算年龄的信息增益了。

![1562141180526](assets/1562141180526.png)





## ID3其局限性主要有以下几点：

- 分支度越高（分类水平越多）的离散变量往往子节点的总信息熵会更小，ID3是按照某一列进行切分，有一些列的分类可能不会对我需要的结果有足够好的指示。极限情况下取ID作为切分字段，每个分类的纯度都是100%，因此这样的分类方式是没有效益的。
- 不能直接处理连续型变量，若要使用ID3处理连续型变量，则首先需要对连续变量进行离散化。
- 对缺失值较为敏感，使用ID3之前需要提前对缺失值进行处理。
- 没有剪枝的设置，容易导致过拟合，即在训练集上表现很好，测试集上表现很差。
- 上面的例子就是已经将特征离散化后的干净数据集



# C4.5

![1562141201301](assets/1562141201301.png)

**选择信息增益率大的产生决策树的节点**

用信息增益比率(gain ratio)来作为选择分支的准则

$g_R(D,A)=g(D,A)/H_A(D)$

分母表示训练数据集D关于特征A的熵

## IV的计算

也称为分支度（IV：Information Value）

$IV=H_A(D)=-\sum_{i=1}^{n}\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}$

n是特征A的取值个数.

这种计算方法其实和经验熵的计算是一致的,只不过经验熵是根据标签进行计算的.而这是根据属性进行计算的。

![img](F:/%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86/%E6%9C%89%E9%81%93%E4%BA%91%E7%AC%94%E8%AE%B0/yangsenupc@163.com/347a7479c8334c01bf5e728a52eafb3f/clipboard.png)

**IV**相当于一个惩罚项,如果以ID进行划分的话,IV值为$log_2|D|$会非常大。所以IV值会随着叶子节点上样本量的变小而逐渐变大，这就是说一个特征中如果标签分类太多，每个叶子上的IV值就会非常大.

## 算法流程

`C4.5` 生成算法与 `ID3` 算法相似，但是 `C4.5` 算法在生成过程中用信息增益比来选择特征

输入：训练数据集$D$，特征$A$，阈值$\varepsilon$  
输出：决策树$T$

1. 若$D$中所有实例属于同一类$C_{k}$，则$T$为单结点树，并将类$C_{k}$作为该结点的类标记，返回$T$； 
2. 若$A = \emptyset$，则$T$为单结点树，并将$D$中实例数最大的类$C_{k}$作为该结点的类标记，返回$T$；
3. 否则，计算$A$中各特征$D$的信息增益，选择信息增益比最大的特征$A_{g}$
    $\begin{align*} \\ & A_{g} = \arg \max_{A} g_{R} \left( D, A \right) \end{align*}  $
4. 如果$A_{g}$的信息增益小于阈值$\varepsilon$，则置$T$为单结点树，并将$D$中实例数量最大的类$C_{k}$作为该结点的类标记，返回$T$;
5. 否则，对$A_{g}$的每一个可能值$a_{i}$，依$A_{g}=a_{i}$将$D$分割为若干非空子集$D_{i}$，将$D_{i}$中实例数对大的类作为标记，构建子结点，由结点及其子结点构成树$T$，返回$T$；
6. 对第$i$个子结点，以$D_{i}$为训练集，以$A-\left\{A_{g}\right\}$为特征集，递归地调用步1.～步5.，得到子树$T_{i}$，返回$T_{i}$。



## 处理连续变量

在C4.5中，同样还增加了针对连续变量的处理手段。如果输入特征字段是连续型变量，则有下列步骤：

1. 算法首先会对这一列数进行从小到大的排序

2. 选取相邻的两个数的中间数作为切分数据集的备选点，若一个连续变量有N个值，则在C4.5的处理过程中将产生N-1个备选切分点，并且每个切分点都代表着一种二叉树的切分方案，例如

    ![1562141223065](assets/1562141223065.png)

这里需要注意的是，此时针对连续变量的处理并非是将其转化为一个拥有N-1个分类水平的分类变量，而是将其转化成了N-1个二分方案，而在进行下一次的切分过程中，这N-1个方案都要单独带入考虑，其中每一个切分方案和一个离散变量的地位均相同（一个离散变量就是一个单独的多路切分方案).



# 决策树的剪枝

1. 决策树生成算法生成的树往往对于训练数据拟合很准确，但是对于未知的测试数据分类却没有那么准确。即出现过拟合现象。

    过拟合产生得原因是决策树太复杂。解决的办法是：对决策树剪枝，即对生成的决策树进行简化。

2. 决策树的剪枝是从已生成的树上裁掉一些子树或者叶结点，并将根结点或者其父结点作为新的叶结点。

    剪枝的依据是：极小化决策树的整体损失函数或者代价函数。

3. 决策树生成算法是学习局部的模型，决策树剪枝是学习整体的模型。即：生成算法仅考虑局部最优，而剪枝算法考虑全局最优。

设树$T$的叶结点个数为$\left| T \right|$，$t$是树$T$的叶结点，该叶结点有$N_{t}$个样本点，其中属于$c_k$类的样本点有$N_{tk}$个，$k=1,2,\cdots,K$，$H_{t}\left(T\right)$为叶结点$t$上的经验熵，  则决策树的损失函数
$$
\begin{align*} \\ & C_{\alpha} \left( T \right) = \sum_{t=1}^{\left| T \right|} N_{t} H_{t} \left( T \right) + \alpha \left| T \right| \end{align*}
$$
其中，$\alpha \geq 0$为参数，经验熵
$$
\begin{align*} \\ & H_{t} \left( T \right) = - \sum_{k} \dfrac{N_{tk}}{N_{t}} \log \dfrac{N_{tk}}{N_{t}} \end{align*}
$$

- 叶结点个数越多，表示决策树越复杂，则损失函数越大。
- 叶结点经验熵越大，表示叶结点的样本类别分布很分散，则损失函数越大。
- 叶结点经验熵还需要加权，权重为叶结点大小。即：越大的叶结点，其分类错误的影响越大。

损失函数中，记
$$
\begin{align*} \\ & C \left( T \right) = \sum_{t=1}^{\left| T \right|} N_{t} H_{t} \left( T \right) = - \sum_{t=1}^{\left| T \right|} \sum_{k=1}^{K} N_{tk} \log \dfrac{N_{tk}}{N_{t}}   \end{align*}
$$
则
$$
\begin{align*} \\ & C_{\alpha} \left( T \right) = C \left( T \right) + \alpha \left| T \right|   \end{align*}
$$
其中，$C \left( T \right)$表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，$\left| T \right|$表示模型复杂度，参数$\alpha \geq 0$控制两者之间的影响。

$C(T)=0$ 意味着 $N_{tk}=N_t$，即每个结点$t$内的样本都是纯的，即单一的分类。

决策树划分得越细致，则树$T$  的叶子结点越多。当叶结点的数量等于样本集的数量时，树 $T$的每个叶子结点只有一个样本点。此时每个叶结点内的样本都是纯的，从而  $C(T)=0$ 。

这样的决策树其实是没有什么实用价值的，所以必须使用正则化项来约束决策树的复杂程度。

参数 $\alpha$控制预测误差与模型复杂度之间的关系。

- 较大的 $\alpha$ 会选择较简单的模型 。
- 较小的$\alpha$  会选择较复杂的模型。
-  $\alpha=0$只考虑对训练集的拟合，不考虑模型复杂度。

决策树剪枝的准则是：考虑当 $\alpha$确定时，$C_\alpha(T)$  最小化。这等价于正则化的极大似然估计。

## 树的剪枝算法：  

输入：决策树$T$，参数$\alpha$ 

输出：修剪后的子树$T_{\alpha}$

1. 计算每个结点的经验熵 

2. 递归地从树的叶结点向上回缩  

    设一组叶结点回缩到其父结点之前与之后的整体树分别为$T_{B}$与$T_{A}$，其对应的损失函数值分别是$C_{\alpha} \left( T_{B} \right)$与$C_{\alpha} \left( T_{A} \right)$，如果
    $$
    \begin{align*} \\ & C_{\alpha} \left( T_{A} \right) \leq C_{\alpha} \left( T_{B} \right)  \end{align*}
    $$
    则进行剪枝，即将父结点变为新的叶结点。

3. 返回2.，直到不能继续为止，得到损失函数最小的子树$T_{\alpha}$



# CART-二叉树

二叉树不是二分类树

## 基尼系数

基尼指数（基尼不纯度）：表示在样本集合中一个随机选中的样本被分错的概率

**注意**： Gini指数越小表示集合中被选中的样本被分错的概率越小，也就是说集合的纯度越高，反之，集合越不纯。

即 **基尼指数（基尼不纯度）= 样本被选中的概率 \* 样本被分错的概率**
$$
\begin{align*} \\ & Gini \left( p \right) = \sum_{k=1}^{K} p_{k} \left( 1 - p_{k} \right) = 1 - \sum_{k=1}^{K}p_{k}^2  \end{align*}
$$
则样本集D的基尼指数为

 (其中$K$为样本的类别总数,当为二分类问题时，K为2；为多分类问题时，K大于2)

$$
Gini(D)=1-\sum_{k=1}^{K}(\frac{|C_k|}{D})^2
$$
根据特征A进行划分

写法1：

![img](F:/%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86/%E6%9C%89%E9%81%93%E4%BA%91%E7%AC%94%E8%AE%B0/yangsenupc@163.com/a4aed60745b740f1ae8f0c9d56420b73/clipboard.png)

写法2：
$$
\begin{align*} \\ & Gini \left( D, A \right) = \dfrac{\left| D_{1} \right|}{\left| D \right|} Gini \left( D_{1} \right) + \dfrac{\left| D_{2} \right|}{\left| D \right|} Gini \left( D_{2} \right)\end{align*}
$$
**这里的A为单个的特征**

**需要说明**的是CART是个二叉树，也就是当使用某个特征划分样本集合只有两个集合：

- 等于给定的特征值 的样本集合$D_1$ 

- 不等于给定的特征值 的样本集合$D_2$

 因而**对于一个具有多个取值（超过2个）的特征，需要计算以每一个取值作为划分点，对样本D划分之后子集的纯度Gini(D,A=s)，(其中A=s 表示特征A的可能取值)**

 然后从所有的可能划分的Gini(D,A=s)中找出Gini指数最小的划分，这个划分的划分点，便是使用特征A对样本集合D进行划分的最佳划分点。

## CART树算法流程

流程中的Aj就是一个特征，可以取不同的值

例如Aj为年龄，可以取到的值为{老，中，青}

![img](F:/%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86/%E6%9C%89%E9%81%93%E4%BA%91%E7%AC%94%E8%AE%B0/yangsenupc@163.com/8c3a676d58304ca093f57d20a9a54819/clipboard.png)

CART 树与ID3 决策树和 C4.5 决策树的重要区别：

- CART 树是二叉树，而后两者是N 叉树。

由于是二叉树，因此 CART 树的拆分不依赖于特征的取值数量。因此CART 树也就不像ID3 那样倾向于取值数量较多的特征。

- CART 树的特征可以是离散的，也可以是连续的。

而ID3的特征是离散的。如果是连续的特征，则需要执行分桶来进行离散化。

CART 树处理连续特征时，也可以理解为二分桶的离散化。



CART树会计算每个特征的每种可能的取值的基尼指数[需要两个循环,第一个循环是取到每个特征，第二个循环是取每个特征的每个可能的取值]，而后两者都是对每个特征进行计算信息增益，而不会取到每个特征中的取值[只有一个循环]

## CART树剪枝

CART剪枝算法：  

输入：CART决策树$T_{0}$   

输出：最优决策树$T_{\alpha}$

1. 设$k=0, T=T_{0}$

2. 设$\alpha=+\infty$

3. 自下而上地对各内部结点$t$计算$ C\left(T_{t}\right),\left| T_{t} \right|$，以及
    $$
    \begin{align*} \\ & g\left(t\right) =  \dfrac{C\left( t \right) - C \left(T_{t}\right)} { \left| T_{t} \right| -1 }
    \\ & \alpha = \min \left( \alpha, g\left( t \right) \right) \end{align*}
    $$
    其中，$T_{t}$表示以$t$为根结点的子树，$ C\left(T_{t}\right)$是对训练数据的预测误差，$\left| T_{t} \right|$是$T_{t}$的叶结点个数。

4. 自下而上地访问内部结点$t$，如果有$g\left(t\right)=\alpha$，则进行剪枝，并对叶结点$t$以多数表决法决定其类别，得到树$T$

5. 设$k=k+1, \alpha_{k}=\alpha, T_{k}=T$

6. 如果$T$不是由根结点单独构成的树，则回到步骤4.

7. 采用交叉验证法在子树序列$T_{0},T_{1},\cdots,T_{n}$中选取最优子树$T_{\alpha}$

![1562141259760](assets/1562141259760.png)



## Classification and Regression Trees, CART

（1）CART既能是分类树，又能是分类树； （2）当CART是分类树时，采用GINI值作为节点分裂的依据； 当CART是回归树时，采用样本的最小方差作为节点分裂的依据； （3）CART是一棵二叉树。 



## **CART如何选择分裂的属性？**

分裂的目的是为了能够让数据变纯，使决策树输出的结果更接近真实值。 那么CART是如何评价节点的纯度呢？如果是分类树，CART采用GINI值衡量节点纯度； 如果是回归树，采用样本方差衡量节点纯度。节点越不纯，节点分类或者预测的效果就越差。 



CART算法流程与C4.5算法相类似： 

- 1若满足停止分裂条件（样本个数小于预定阈值，或Gini指数小于预定阈值 （样本基本属于同一类，或没有特征可供分裂），则停止分裂；

-  2否则，选择最小Gini指数进行分裂；

-  3递归执行1-2步骤，直至停止分裂。 

![1562141278478](assets/1562141278478.png)





![img](F:/%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86/%E6%9C%89%E9%81%93%E4%BA%91%E7%AC%94%E8%AE%B0/yangsenupc@163.com/7a8f59e5506249729e5ae50e6e7bc0ca/clipboard.png)



首先我们看看决策树算法的优点：

1）简单直观，生成的决策树很直观。

2）基本不需要预处理，不需要提前归一化，处理缺失值。

3）使用决策树预测的代价是*O*(*log*2*m*)。 m为样本数。

4）既可以处理离散值也可以处理连续值。很多算法只是专注于离散值或者连续值。

5）可以处理多维度输出的分类问题。

6）相比于神经网络之类的黑盒分类模型，决策树在逻辑上可以得到很好的解释

7）可以交叉验证的剪枝来选择模型，从而提高泛化能力。

8） 对于异常点的容错能力好，健壮性高。

我们再看看决策树算法的缺点:

1）决策树算法非常容易过拟合，导致泛化能力不强。可以通过设置节点最少样本数量和限制决策树深度来改进。

2）决策树会因为样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习之类的方法解决。

3）寻找最优的决策树是一个NP难的问题，我们一般是通过启发式方法，容易陷入局部最优。可以通过集成学习之类的方法来改善。

4）有些比较复杂的关系，决策树很难学习，比如异或。这个就没有办法了，一般这种关系可以换神经网络分类方法来解决。

5）如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。这个可以通过调节样本权重来改善

## CART树计算

![1562141297352](assets/1562141297352.png)



以属性“职业”为例，一共有三个离散值，“学生”、“老师”、“上班族”。该属性有三种划分的方案，分别为{“学生”}、{“老师”、“上班族”}，{“老师”}、{“学生”、“上班族”}，{“上班族”}、{“学生”、“老师”}，分别计算三种划分方案的子节点GINI值或者样本方差，选择最优的划分方法，如下图所示：

**第一种划分方法：{“学生”}、{“老师”、“上班族”}**

![1562141313971](assets/1562141313971.png)

![1562141380311](assets/1562141380311.png)

预测是否已婚（分类）：

![1562141397375](assets/1562141397375.png)

预测年龄（回归）：

![1562141414569](assets/1562141414569.png)

**第二种划分方法：{“老师”}、{“学生”、“上班族”}**

![1562141332149](assets/1562141332149.png)

 

预测是否已婚（分类）：

   ![1562141348684](assets/1562141348684.png)                 

预测年龄（回归）：

​      　　　　  ![1562141429574](assets/1562141429574.png)

**第三种划分方法：{“上班族”}、{“学生”、“老师”}**

![1562141447262](assets/1562141447262.png)

 预测是否已婚（分类）：

![1562141462769](assets/1562141462769.png)

预测年龄（回归）：

![1562141479933](assets/1562141479933.png)

综上，如果想预测是否已婚，则选择{“上班族”}、{“学生”、“老师”}的划分方法，如果想预测年龄，则选择{“老师”}、{“学生”、“上班族”}的划分方法。



# 剪枝处理

从决策树的构造流程中我们可以直观地看出：不管怎么样的训练集，决策树总是能很好地将各个类别分离开来，这时就会遇到之前提到过的问题：过拟合（overfitting），即太依赖于训练样本。剪枝（pruning）则是决策树算法对付过拟合的主要手段，剪枝的策略有两种如下：

* 预剪枝（prepruning）：在构造的过程中先评估，再考虑是否分支[设定剪枝的阈值]。
* 后剪枝（post-pruning）：在构造好一颗完整的决策树后，自底向上，评估分支的必要性。

评估指的是性能度量，即决策树的泛化性能。之前提到：可以使用测试集作为学习器泛化性能的近似，因此可以将数据集划分为训练集和测试集。预剪枝表示在构造数的过程中，对一个节点考虑是否分支时，首先计算决策树不分支时在测试集上的性能，再计算分支之后的性能，若分支对性能没有提升，则选择不分支（即剪枝）。后剪枝则表示在构造好一颗完整的决策树后，从最下面的节点开始，考虑该节点分支对模型的性能是否有提升，若无则剪枝，即将该节点标记为叶子节点，类别标记为其包含样本最多的类别。

![8.png](https://i.loli.net/2018/10/17/5bc728ec80d34.png)

![9.png](https://i.loli.net/2018/10/17/5bc728ec9e330.png)

![10.png](https://i.loli.net/2018/10/17/5bc728ec9d497.png)

上图分别表示不剪枝处理的决策树、预剪枝决策树和后剪枝决策树。预剪枝处理使得决策树的很多分支被剪掉，因此大大降低了训练时间开销，同时降低了过拟合的风险，但另一方面由于剪枝同时剪掉了当前节点后续子节点的分支，因此预剪枝“贪心”的本质阻止了分支的展开，在一定程度上带来了欠拟合的风险。而后剪枝则通常保留了更多的分支，因此采用后剪枝策略的决策树性能往往优于预剪枝，但其自底向上遍历了所有节点，并计算性能，训练时间开销相比预剪枝大大提升。

#  连续值与缺失值处理

对于连续值的属性，若每个取值作为一个分支则显得不可行，因此需要进行离散化处理，常用的方法为二分法，基本思想为：给定样本集D与连续属性α，二分法试图找到一个划分点t将样本集D在属性α上分为≤t与＞t。

* 首先将α的所有取值按升序排列，所有相邻属性的均值作为候选划分点（n-1个，n为α所有的取值数目）。
* 计算每一个划分点划分集合D（即划分为两个分支）后的信息增益。
* 选择最大信息增益的划分点作为最优划分点。

![11.png](https://i.loli.net/2018/10/17/5bc72a0968fad.png)

现实中常会遇到不完整的样本，即某些属性值缺失。有时若简单采取剔除，则会造成大量的信息浪费，因此在属性值缺失的情况下需要解决两个问题：（1）如何选择划分属性。（2）给定划分属性，若某样本在该属性上缺失值，如何划分到具体的分支上。假定为样本集中的每一个样本都赋予一个权重，根节点中的权重初始化为1，则定义：

![12.png](https://i.loli.net/2018/10/17/5bc72a098f3be.png)

对于（1）：通过在样本集D中选取在属性α上没有缺失值的样本子集，计算在该样本子集上的信息增益，最终的信息增益等于该样本子集划分后信息增益乘以样本子集占样本集的比重。即：

![13.png](https://i.loli.net/2018/10/17/5bc72a096ccc3.png)

对于（2）：若该样本子集在属性α上的值缺失，则将该样本以不同的权重（即每个分支所含样本比例）划入到所有分支节点中。该样本在分支节点中的权重变为：

![14.png](https://i.loli.net/2018/10/17/5bc72a093ed3c.png)





# 决策树可视化

```python
import pydotplus
from sklearn.tree import DecisionTreeClassifier, export_graphviz
model = DecisionTreeClassifier()
model.fit(x_train, y_train.astype('int'))
# 输出决策树可视化文件
dot_data = export_graphviz(model, out_file=None, feature_names=list(dataframe.columns)[:6])
graph = pydotplus.graph_from_dot_data(dot_data)
graph.write_pdf('./tree.pdf')
#Image(graph.create_png())  #jupyter里可以显示，pycharm显示不出
graph.write_png(r'DT_show.png')
```



```python
# 第五章中的决策树
from sklearn.tree import DecisionTreeClassifier as DTC
dtc = DTC(criterion='entropy')  # 建立决策树模型，基于信息熵
dtc.fit(x, y)  # 训练模型

# 导入相关函数，可视化决策树。
# 导出的结果是一个dot文件，需要安装Graphviz才能将它转换为pdf或png等格式。
from sklearn.tree import export_graphviz
with open("tree.dot", 'w') as f:
    f = export_graphviz(dtc, feature_names=x.columns, out_file=f)

# show tree.dot
# $dot -Tpdf tree.dot -o tree.pdf  转换成pdf文件,但中文无法识别

```



```python
# 菜菜sklearn中的决策树
feature_name = ['酒精','苹果酸','灰','灰的碱性','镁','总酚','类黄酮','非黄烷类酚类','花青素','颜色强度','色调','od280/od315稀释葡萄酒','脯氨酸']
import graphviz
dot_data = tree.export_graphviz(clf
	,out_file = None
	,feature_names= feature_name
	,class_names=["琴酒","雪莉","贝尔摩德"]
	,filled=True
	,rounded=True
	)
graph = graphviz.Source(dot_data)
```

# 李航课后习题



![1559791172073](assets\1559791172073.png)